---
title: "Graph"
date: 2021-04-30
categories:
  - blog
tags:
  - algorithm
  - graph
  - summary
---

1. Definitions:
    1. A graph is connected if there is a path from every vertex to every other vertex in the graph.
    2. Acyclic graph: a graph with no cycles.
    3. Tree is an acyclic connected graph.    
    4. Spanning Tree: A spanning tree of a graph is a connected subgraph with no cycles that include all vertices.
    5. For simple graph (at most one edge between any pair of vertices), the asympotic relationship between `V` and `E` is: `E=O(V^2)`, the upper bound is achieved in a complete graph, think about a matrix where each element `m[i][j] = 1` indicates an edge between node `i` and `j`.
    6. Adjacency-lists (linked list) data structure is used to represent the edges for vertex.
    7. A directed acyclic graph (DAG) is a digraph with no directed cycles.

2. [图节点的逻辑结构][图论基础]
    ```
    /* 图节点的逻辑结构 */
    class Vertex {
        int id;
        Vertex[] neighbors;
    ```
    2. 图真的没啥高深的，本质上就是个高级点的多叉树而已，适用于树的 DFS/BFS 遍历算法，全部适用于图。
    3. 对于邻接表，好处是占用的空间少。但是，邻接表无法快速判断两个节点是否相邻。但对于邻接矩阵就简单了, 效率高
    4. 图和多叉树最大的区别是，图是可能包含环的，你从图的某一个节点开始遍历，有可能走了一圈又回到这个节点。所以，如果图包含环，遍历框架就要一个 visited 数组进行辅助.
    ```
    // 记录被遍历过的节点
    boolean[] visited;
    // 记录从起点到当前节点的路径
    boolean[] onPath;

    /* 图遍历框架 */
    void traverse(Graph graph, int s) {
        if (visited[s]) return;
        // 经过节点 s，标记为已遍历
        visited[s] = true;
        // 做选择：标记节点 s 在路径上
        onPath[s] = true;
        for (int neighbor : graph.neighbors(s)) {
            traverse(graph, neighbor);
        }
        // 撤销选择：节点 s 离开路径
        onPath[s] = false;
    }
    ```
    5. 类比贪吃蛇游戏，visited 记录蛇经过过的格子，而 onPath 仅仅记录蛇身。onPath 用于判断是否成环，类比当贪吃蛇自己咬到自己（成环）的场景。如果让你处理路径相关的问题，这个 onPath 变量是肯定会被用到的，比如 拓扑排序 中就有运用。你应该注意到了，这个 onPath 数组的操作很像回溯算法核心套路 中做「做选择」和「撤销选择」，区别在于位置：回溯算法的「做选择」和「撤销选择」在 for 循环里面，而对 onPath 数组的操作在 for 循环外面。在 for 循环里面和外面唯一的区别就是对根节点的处理。前者会正确打印所有节点的进入和离开信息，而后者唯独会少打印整棵树根节点的进入和离开信息。为什么回溯算法框架会用后者？因为回溯算法关注的不是节点，而是树枝。









2. Directed Graph (Digraph)
    1. Reverse a digraph
    2. Multiple-source reachability. Given a digraph and a set of source vertices, support queries of the form "Is there a directed path from any vertex in the set to a given target vertex v?" (Mark-and-sweep garbage collection)
        * DFS

2. Depth-First Search (DFS)
    1. Recursive:
        * Complexity: DFS marks all the vertices connected to a given source in time proportional to the sum of their degrees (`2E`). 
            * To run DFS on all vertices of a graph (so we need visit each vertex once at least), the complexity is `V+E`.
        * We encounter each edge in the graph twice (once at each of its vertices)
        * Careful not to repeat vertices
        * N.B. It's possible that we'll get stackoverflow when using recursion method. If that can be the case, use the stack version instead.
        ```
        def DepthFirstSearch(adjacency_list, s: 'Node'):
            marked = {}
            dfs(adjacency_list, s)
        
        def dfs(adjacency_list, start_node: 'Node', marked):
            marked[start_node] = True
            count += 1 #some action            
            for w in adjacency_list[start_node].neighbors:
                if not marked[w]:
                    dfs(adjacency_list, w, marked)   
        
        # A 2D matrix example to find the islands marked by '1'. If the matrix is MxN, then for this graph there can be a max of MxN vertices, and max of M*(N-1)+N*(M-1) edges. 

        m, n = len(grid), len(grid[0])
        visited = set() 
        # The max length of system stack can be O(MxN) if every node is '1'.   
        def dfs(x, y):            
            visited.add((x, y))
            for (dx, dy) in [[0,1], [0, -1], [1, 0], [-1, 0]]:
                nx, ny = x+dx, y+dy
                if nx >= 0 and nx < m and ny >= 0 and ny < n and grid[nx][ny] == '1' and (nx, ny) not in visited:                    
                    dfs(nx, ny)        

        res = 0
        for i in range(m):
            for j in range(n):
                if grid[i][j] == '0' or (i,j) in visited:
                    continue                
                dfs(i, j)
                res += 1
        
        return res        
        ```
        * Problems
            * [LC200. Number of Islands][LC200. Number of Islands]

    2. Uses LIFO stack: 
    ```
    stack = [start_node]
    visited = set([start_node])    
    while stack:
        current = stack.pop()        
        # N.B. Avoid putting the same node onto stack twice. so we need add a node into the visited set  # right before we add it into the stack
        # It won't work if put visited.add(current) here. For example, If node A, B, C are in the stack 
        # and `current = A`, if B is A's neighbor, since B is not in the visited set, we are going to add # B into the stack, which is wrong since B is already on stack. Think about a graph of 3 nodes    # with triangle shape.

        for neighbor in current.neighbors:
            if not neighbor in visited:
                visited.add(neighbor)
                stack.append(neighbor)    

        # A 2D Matrix Example:
        m, n = len(grid), len(grid[0])
        visited = set()

        # What's the maximum length of stack here ? 
        def dfs(x, y):            
            stack = [(x,y)]
            visited.add((x,y))
            while stack:
                cx, cy = stack.pop()                
                for (dx, dy) in [[0,1], [0, -1], [1, 0], [-1, 0]]:
                    nx, ny = cx+dx, cy+dy
                    if nx >= 0 and nx < m and ny >= 0 and ny < n and grid[nx][ny] == '1' and (nx, ny) not in visited:                    
                        visited.add((nx,ny))
                        stack.append((nx,ny))
                
        res = 0
        for i in range(m):
            for j in range(n):
                if grid[i][j] == '0' or (i,j) in visited:
                    continue                
                dfs(i, j)
                res += 1
        
        return res                    
    ```
    3. Single-source connectivity: Are two given vertices connected (path detection) ? and How many connected components does the graph have ?
        * DFS: `V+E`
        * Union-Find: however it can't find the path
            * It's an online algorithm: we can check whether two vertices are connected in near-constant time at any point, even while adding edges
    4. Single-source paths (Finding Paths). Given a graph and a source vertex s, support queries of the form "Is there a path from s to a given target vertex v? If so, find such a path".
        * DFS
            * It maintains a vertex-indexed array `edgeTo[]` or `parent[]` such that `edgeTo[w] = v` means that `v-w` was the edge used to access `w` for the first time. The `edgeTo[]` array is a parent-link representation of a tree rooted at `s` that contains all the vertices connected to `s`. Instead of just keeping track of the path from the current vertex back to the start
            * The paths discovered by depth-first search depend not just on the graph, but also on the representation and the nature of the recursion.    
    
    4. [Two-colorability][LC785. Is Graph Bipartite?]: Can the vertices of a given graph be assigned one of two colors in such a way that no edge connects vertices of the same color ? which is equivalent to this question: Is the graph bipartite ?
        * 从简单实用的角度来看，二分图结构在某些场景可以更高效地存储数据。如果用哈希表存储，需要两个哈希表分别存储「每个演员到电影列表」的映射和「每部电影到演员列表」的映射。但如果用「图」结构存储，将电影和参演的演员连接，很自然地就成为了一幅二分图：每个电影节点的相邻节点就是参演该电影的所有演员，每个演员的相邻节点就是该演员参演过的所有电影，非常方便直观。所以在某些场景下图结构也可以作为存储键值对的数据结构（符号表）。
        * 
    5. Edge Classification
        * Tree edges: (parent pointer in DFS), the edges that are used in DFS to visit new vertex. They form a tree (and forest).
        * Forward edges
        * Backward edges: if a vertex is already on the stack, then current edge is back edge
        * Cross edges
        * In an undirected graph, only tree edges and backward edges can exist. There are no forward/cross edges.

3. Breadth-First-Search (BFS)
    1. The goal is to visit all the nodes reachable from a given source node `s`
        1. It uses a FIFO queue: 
        ```
        # Space complexity : O(min(M, N)) because in worst case where the grid is filled with lands, the size of queue can grow up to min(M,N). N.B. It's not max(M,n) here.

        m, n = len(grid), len(grid[0])
        visited = set()
        def bfs(x, y):            
            dq = deque([(x,y)])
            visited.add((x,y))
            while dq:
                sz = len(dq)
                for i in range(sz):
                    cx, cy = dq.popleft()                
                    for (dx, dy) in [[0,1], [0, -1], [1, 0], [-1, 0]]:
                        nx, ny = cx+dx, cy+dy
                        if nx >= 0 and nx < m and ny >= 0 and ny < n and grid[nx][ny] == '1' and (nx, ny) not in visited:                    
                            visited.add((nx,ny))
                            dq.append((nx,ny))
                
        res = 0
        for i in range(m):
            for j in range(n):
                if grid[i][j] == '0' or (i,j) in visited:
                    continue                
                bfs(i, j)
                res += 1
        
        return res        
        ```
        2. BFS takes time proportional to `O(V+E)` in the worst case. You only check each edge once (`|E|` in directed graph) or twice (`2|E|` in undirected graph), and a vertex enters the queue only once. 
        3. Need to be carful to avoid duplicates by using a 'visited' set
            * Be careful on where to add the vertex into the 'visited' set though. 
            * Shall we add a vertex into the 'visited' set when we pop it out of the queue?
            * Shall we add a neighbor of a vertex into the 'visited' set before appending it onto the queue while we loop through a vertex's neighbors? 
            * The latter is more efficient as we avoid adding many more vertices into the queue. If we use the former, the distance between each visited vertex can be much larger. See [Minimum Knight Moves][LC1197 Minimum Knight Moves].
        4. "Pruning" can be used to improve the efficency of BFS. e.g. We can use bidirectional BFS instead of unidirectional BFS. or We can remove unwanted vertices from the queue.

4. Minimum Spanning Tree (MST) of Edge-Weighted Undirected Graph
    1. [先说「树」和「图」的根本区别：树不会包含环，图可以包含环。][KRUSKAL 最小生成树算法]. 如果一幅图没有环，完全可以拉伸成一棵树的模样。说的专业一点，树就是「无环连通图」。一般来说，我们都是在无向加权图中计算最小生成树的，所以使用最小生成树算法的现实场景中，图的边权重一般代表成本、距离这样的标量。
    2. 所谓最小生成树，就是图中若干边的集合，你要保证这些边：
        1. 包含图中的所有节点。
        2. 形成的结构是树结构（即不存在环）。
        3. 权重和最小。
    0. A minimum spanning tree of an edge-weighted graph is a spanning tree whose weight (the sum of the weights of its edges) is no larger than the weight of any other spanning tree.
    1. Adding an edge that connects two vertices in a tree creates a unique cycle
    2. Removing an edge from a tree breaks it into two separate subtrees
    3. Cut Property: A cut of a graph is a partition of its vertices into two nonempty disjoint sets. A crossing edge of a cut is an edge that connects a vertex in one set with a vertex in the other.
        * Given any cut in an edge-weighted graph, the crossing edge of minimum weight is in the MST of the graph.
        * Note that there is no requirement that the minimal edge be the only MST edge connecting the two sets; indeed, for typical cuts there are several MST edges that connect a vertex in one set with a vertex in the other
    4. 「切分」就是将一幅图分为两个不重叠且非空的节点集合
    5. 「切分定理」：对于任意一种「切分」，其中权重最小的那条「横切边」一定是构成最小生成树的一条边。
    6. The edge weights are all different. If edges can have equal weights, the minimum spanning tree may not be unique.
    7. Greedy Algorithm
        1. The following method colors black all edges in the the MST of any connected edge-weighted graph with `V` vertices: starting with all edges colored gray, find a cut with no black edges, color its minimum-weight edge black, and continue until `V-1` edges have been colored black.
        2. Prim's Algorithm
            1. 首先，Prim 算法也使用贪心思想来让生成树的权重尽可能小，也就是「切分定理」. 既然每一次「切分」一定可以找到最小生成树中的一条边，那我就随便切呗，每次都把权重最小的「横切边」拿出来加入最小生成树，直到把构成最小生成树的所有边都切出来为止。
            2. 其次，Prim 算法使用 BFS 算法思想 和 visited 布尔数组避免成环，来保证选出来的边最终形成的一定是一棵树。
                * 在进行切分的过程中，我们只要不断把新节点的邻边加入横切边集合，就可以得到新的切分的所有横切边。当然，细心的读者肯定发现了，cut({A, B}) 的横切边和 cut({C}) 的横切边中 BC 边重复了。不过这很好处理，用一个布尔数组 inMST 辅助，防止重复计算横切边就行了。
            3. Kruskal 算法是在一开始的时候就把所有的边排序，然后从权重最小的边开始挑选属于最小生成树的边，组建最小生成树。
            4. Prim 算法是从一个起点的切分（一组横切边）开始执行类似 BFS 算法的逻辑，借助切分定理和优先级队列动态排序的特性，从这个起点「生长」出一棵最小生成树。
            1. Use priority queue to hold crossing edges and find the cut with minimum weight
            2. We can use eager algorithm, we maintain on the priority queue just one edge for each non-tree vertex `w`: the shortest edge that connects it to the tree.
            3. Time complexity: `O(ElogE)` (lazy version), `O(ElogV)` (eager version). 复杂度主要在优先级队列 pq 的操作上，由于 pq 里面装的是图中的「边」，假设一幅图边的条数为 E，那么最多操作 O(E) 次 pq。每次操作优先级队列的时间复杂度取决于队列中的元素个数，取最坏情况就是 O(logE)。所以这种 Prim 算法实现的总时间复杂度是 O(ElogE)
            4. The process of building MST consists of a loop with the following substeps:
                * Each iteration, we pop an element from the heap. This element contains a vertex along with the cost that is associated with the edge that connecting the vertex to the tree. The vertex is chosen if it is not already in the tree. We know that the cost of this vertex is minimal among all choices because it was popped from the heap.
                * Once the vertex is added, we then examine its neighboring vertices. Specifically, we add these vertices along with their edges into the heap as the candidates for the next round of selection.
                * The loop terminates when we have added all the vertices from the graph into the MST.
                * NB. You can use `defaultdict(list)` to store both the adjacency list and edge weights like this: `graph[n0] = [(w1, n1), (w3, n3)]`.
                * NB. It's possible that even for undirectional graph, there can be two edges between a pair of nodes, e.g. `weight(n5, n6) = 7` and `weight(n6, n5) = 8`. If such case is possible, we should store the edge weights in both directions.
        3. Kruskal's Algorithm
            0. 将所有边按照权重从小到大排序，从权重最小的边开始遍历，如果这条边和mst中的其它边不会形成环，则这条边是最小生成树的一部分，将它加入mst集合；否则，这条边不是最小生成树的一部分，不要把它加入mst集合。
            1. It processes the edges in order of their weight values (smallest to largest), taking for the MST (coloring black) each edge that does not form a cycle with edges previously added, stopping after adding `V-1` edges have been taken.
                * A more concise criteria to determine whether we should add a new edge in Kruskal's algorithm is that whether both ends of the edge belong to the same component (group).
            2. It uses a priority queue to consider the edges in order of weight
            3. It uses **union-find** data structure to identify those that cause cycles.
            4. 假设一幅图的节点个数为V，边的条数为E，首先需要O(E)的空间装所有边，而且 Union-Find 算法也需要O(V)的空间，所以 Kruskal 算法总的空间复杂度就是O(V + E)。时间复杂度主要耗费在排序，需要O(ElogE)的时间，Union-Find 算法所有操作的复杂度都是O(1)，套一个 for 循环也不过是O(E)，所以总的时间复杂度为O(ElogE)。
    6. Related Problems
        1. [Optimize Water Distribution in a Village][LC1168 Optimize Water Distribution in a Village]
        2. 

5. Cycles 
    0. A graph `G` has a cycle if and only if the depth-first search of `G` has a back edge.
    1. Cycle detection in un-directed graph
        * The key is to avoid going along edges we've already been on (in the opposite direction)
        * While traverse the graph, remove the opposite edge. if you encounter any node that has been visited before, there's a cycle. `O(V+E)`
        ```
        graph = defaultdict(set)
        for (n1, n2) in edges:
            graph[n1].add(n2)
            graph[n2].add(n1)

        is_valid = True
        visited = set([0])
        def dfs(start):
            nonlocal is_valid
            for neighbor in graph[start]:
                if neighbor not in visited:
                    graph[neighbor].remove(start)
                    visited.add(neighbor)
                    dfs(neighbor)
                else:                    
                    is_valid = False
                    return
            
        dfs(0)        
        return is_valid and len(visited) == n        
        ```
        * Another method is to record each node's parent node in the visited dictionary.  `O(V+E)`
            * Ignore the parent node of current node's neighbor
            * If still, the neighbor node is one of the already visited parents, then there's a cycle.
        ```
        visited = {0:-1}
        stack = [0]
        while stack:
            current = stack.pop()
            for neighbor in graph[current]:
                if neighbor not in visited:
                    visited[neighbor] = current
                    stack.append(neighbor)
                elif visited[current] != neighbor:   #the neighbor is the parent of current node
                    return False
                    
        return len(visited) == n        
        ```
        * graph theory: For the graph to be a valid tree, it must have exactly `n - 1` edges. Any less, and it can't possibly be fully connected. Any more, and it has to contain cycles. Additionally, if the graph is fully connected and contains exactly `n - 1` edges, it can't possibly contain a cycle, and therefore must be a tree!
            * Checking whether or not a graph is fully connected: if all nodes were reachable from a search starting at a single node.
                * you can use DFS or BFS and compare visited nodes with total number of nodes
        * You can also use breadth-first-search
        * Union Find: 
            * Each time there was no merge between two sets, it was because we were adding an edge between two nodes that were already connected via a path. This means there is now an additional path between them—which is the definition of a cycle. 
            * Union find represents each of the sets as a directed tree, with the edges pointing towards the root node.
    2. Cycle detection in directed graph
        * Does a given digraph have a directed cycle? If so, find the vertices on some such cycle, in order from some vertex back to itself. i.e. Is it a DAG? 
        * Backtracking/DFS: put a node in the visited path, check, then remove the node from visited path.
            * Postorder DFS: It uses a boolean array `onStack[]` to keep track of the vertices for which the recursive call has not completed. When it finds a destination vertex `w`, from edge `v->w`, is on the stack, it has discovered a directed cycle, which it can recover by following `edgeTo[]` links. `O(E+V)`, space `O(E+V)`.
            * Worst case: all nodes are chained in a line, and we need check for each node
            ```
            def dfs(course, stack, visited):    
                stack.append(course)
                visited.add(course)
                for neighbor in graph[course]:
                    if neighbor not in visited:
                        if not dfs(neighbor, stack, visited)
                            return False
                    elif neighbor in stack:
                        return False
                stack.pop()
                return True
                        
            visited = set()
            for course in range(numCourses):            
                if course not in visited and not dfs(course, [], visited):
                    return False        
            return True
            ```        
        * The node-coloring variant of the algorithm, The idea is to do DFS of a given graph and while doing traversal, assign one of the below three colors to every vertex.
            * WHITE ~ Vertex is not processed yet. Initially, all vertices are WHITE.
            * GRAY ~ Vertex is being processed (DFS for this vertex has started, but not finished which means that all descendants (in DFS tree) of this vertex are not processed yet (or this vertex is in the function call stack).
            * BLACK ~ Vertex and all its descendants are processed.
            * While doing DFS, if an edge is encountered from current vertex to a GRAY vertex, then this edge is a back edge and hence there is a cycle. A GRAY node represents a node whose processing is still ongoing. Thus, if a descendent eventually leads back to a node whose processing is ongoing, it ends up creating a cycle in the directed graph and we call the edge leading back to a GRAY node as a backward edge.
            * There are three possibilities for the color of this current node.
                * If it is BLACK, do nothing; we've already processed it.
                * If it is WHITE, then mark it as GRAY since we are starting the processing of the graph rooted at this node.
                * Otherwise, if it is GRAY, then return false as we have discovered a backward edge, and hence a cycle.
                ```
                def dfs(course, nodes):                
                    nodes[course] = 1
                    for neighbor in graph[course]:
                        if nodes[neighbor] == 1 or (nodes[neighbor] == 0 and not dfs(neighbor, nodes)):
                            return False
                    nodes[course] = 2
                    return True
                            
                nodes = {i:0 for i in range(numCourses)} #white: 0, gray: 1, black: 2
                for course in range(numCourses):            
                    if nodes[course] == 0 and not dfs(course, nodes):
                        return False        
                return True            
                ```
        * Why not Breadth-First Search?
            * From [this Stack Overflow answer][why-dfs-and-not-bfs-for-finding-cycle-in-graphs]:
            * A BFS could be reasonable if the graph is undirected, where each cross edge defines a cycle (edge going from a node to an already visited node). If the cross edge is `{v1, v2}`, and the root (in the BFS tree) that contains those nodes is `r`, then the cycle is `r ~ v1 - v2 ~ r` (`~` is a path, `-` a single edge), which can be reported almost as easily as in DFS.
            * The only reason to use a BFS would be if you know your (undirected) graph is going to have long paths and small path cover (in other words, deep and narrow). In that case, BFS would require proportionally less memory for its queue than DFS' stack (both still linear of course).
            * In all other cases, DFS is clearly the winner.
        * Use rank (LC1192)
            * A critical connection is a connection that, if removed, will make some node unable to reach some other node in a graph.
            * An edge is a critical connection, if and only if it is not in a cycle.
        * 让你不仅要判断是否存在环，还要返回这个环具体有哪些节点，怎么办？Save the node that points to current node
6. Single-Source Shortest Path
    1. The **Single-source shortest paths** problem: Given a graph and a source vertex `s`, "Is there a path from `s` to a given target vertex `v`? If so, find a shortest such path (one with a minimal number of edges or minimal total weights, not necessarily the only one)"
        * Optimal substructure: Subpaths of a shortest path are shortest paths
    2. General Structure of solution (no negative cycles)
        * Initialize: set source distance `d[s]=0`, and for each vertex `u`, set distance `d[u]`(The length of the current shortest path from source `s` to `u`) to `infinity`, and predecessor `p[u]` (The predecessor of `u` in the shortest path from `s` to `u`) as `None`
        * Repeat: select some edge `(u,v)` in some way (Algorithm specific)
        * **Relaxation** of `edge(u,v)` (from `u` to `v`): 
        ```
        if d[v] > d[u] + w(u,v): #distance from s to v is larger than the distance of s->u->v
            d[v] = d[u] + w(u,v)
            p[v] = u #predecessor
        ```
        * Until all edges have `d[v] <= d[u] + w(u,v)` (brutal force)
        * This algorithm depends on the ordering of vertices we choose which can be exponentially time complexity
    3. Algorithms
        * BFS with deque for unweighted graph, `O(V+E)`
            * BFS calculates the shortest paths in unweighted graph (or weighted graph with all weights equal to 1). It can maintain `edgeTo[]` or `parent[]` to save the paths
        * Topological sort for DAG (directed, no-cycle), `O(V+E)`: 
        * Dijkstra algorithm for nonnegatively weighted, cyclic graph, `O(V + ElogV)`: 
            * Solves the **single-source shortest-paths problem in edge-weighted digraphs with nonnegative weights**.
        * Bellmen-Ford Algorithm for negatively weighted graph, `O(VE)`
            * Works on edge with both positive and negative weights
            * Negative weight cycle: need detect and mark the related vertices with infinity cost
    4. BFS for the shortest distance in unweighted graph
        * If we reached a node that has been visited before, we should ignore it because we must have reached it with a shorter or equal path before.
        * Template
        ```
        dq = deque([(1, beginWord)])
        seen= set([beginWord])
        while dq:
            dist, word = dq.popleft()
            if word == endWord:
                return dist
            for neigh in graph[word]:
                if neigh not  in seen:
                    seen.add(neigh)
                    dq.append((dist+1, neigh))
        return 0        
        ```
        * To output all shortest paths
            * In this case, we need to be careful when use `visited` set, if we add a node into `visited` when it's a neighbor of current node, it's wrong because another shorter path may also pass this node, which will be skipped in this case. For example, tree: `{x->z, y->z, x->y}`, `z` won't be added to the path starting with `y` if `x` is visited first and `z` is already seen in `x`'s search.
            * We can save the currently computed distance in a dictionary from a node to the source. In the beginning, set all distances to `math.inf`. In the deque, we put a tuple of `(distance, node, current_path)` there. As long as we see that the current node is the target node, we put the `current_path` into result. For each neighbor of the current node, if we see that the current distance, which is `distance+1` is less or equal to the previously computed distance, we update the dictionary, and put the neighbor onto deque.
            * We still put the `current_path` onto the deque. At the same time, we add the nodes that are appended onto the deque into `visited` set after we finish processing a level.
            * Use BFS to construct a DAG (on each layer, record the neighbors put on the deque, remove them before processing them as next layer from queue). The DAG will restrict backward paths, all paths to the target node will have the shortest distance. Then use backtrack to find all paths. N.B. If two nodes point to the same node, we may put the same node twice onto the queue. It'll produce duplicate edges coming out of this common node. To avoid this, use a set to record the nodes that have been put on the queue. Or use `defaultdict(set)` for the graph to avoid duplicate edges. 
        * Breadth-first search has no way of knowing if a particular discovery of a node would give us the shortest path to that node. And so, the only possible way for BFS (or DFS) to find the shortest path in a weighted graph is to search the entire graph and keep recording the minimum distance from source to the destination vertex.
    5. Dijkstra Algorithm (Greedy) with nonnegative weights
        * [Dijkstra 算法无非就是一个 BFS 算法的加强版，它们都是从二叉树的层序遍历衍生出来的][我写了一个模板，把 DIJKSTRA 算法变成了默写题].
        * 所谓 BFS 算法，就是把算法问题抽象成一幅「无权图」，然后继续玩二叉树层级遍历那一套罢了。注意，我们的 BFS 算法框架也是 while 循环嵌套 for 循环的形式，也用了一个 step 变量记录 for 循环执行的次数，无非就是多用了一个 visited 集合记录走过的节点，防止走回头路罢了。所谓「无权图」，与其说每条「边」没有权重，不如说每条「边」的权重都是 1，从起点 start 到任意一个节点之间的路径权重就是它们之间「边」的条数，那可不就是 step 变量记录的值么？再加上 BFS 算法利用 for 循环一层一层向外扩散的逻辑和 visited 集合防止走回头路的逻辑，当你每次从队列中拿出节点 cur 的时候，从 start 到 cur 的最短权重就是 step 记录的步数。
        * Dijkstra 算法的时间复杂度是多少？你去网上查，可能会告诉你是 O(ElogV)，其中 E 代表图中边的条数，V 代表图中节点的个数。因为理想情况下优先级队列中最多装 V 个节点，对优先级队列的操作次数和 E 成正比，所以整体的时间复杂度就是 O(ElogV)。不过这是理想情况，Dijkstra 算法的代码实现有很多版本，不同编程语言或者不同数据结构 API 都会导致算法的时间复杂度发生一些改变。比如本文实现的 Dijkstra 算法，使用了 Java 的 PriorityQueue 这个数据结构，这个容器类底层使用二叉堆实现，但没有提供通过索引操作队列中元素的 API，所以队列中会有重复的节点，最多可能有 E 个节点存在队列中。所以本文实现的 Dijkstra 算法复杂度并不是理想情况下的 O(ElogV)，而是 O(ElogE)，可能会略大一些，因为图中边的条数一般是大于节点的个数的。
        * 使用 Dijkstra 算法的前提，加权有向图 (无向图本质上可以认为是「双向图」，从而转化成有向图。)，没有负权重边，求最短路径，OK，可以使用，咱们来套框架。
        ```
        class State {
            // 图节点的 id
            int id;
            // 从 start 节点到当前节点的距离
            int distFromStart;

            State(int id, int distFromStart) {
                this.id = id;
                this.distFromStart = distFromStart;
            }
        }

        // 返回节点 from 到节点 to 之间的边的权重
        int weight(int from, int to);

        // 输入节点 s 返回 s 的相邻节点
        List<Integer> adj(int s);

        // 输入一幅图和一个起点 start，计算 start 到其他节点的最短距离
        int[] dijkstra(int start, List<Integer>[] graph) {
            // 图中节点的个数
            int V = graph.length;
            // 记录最短路径的权重，你可以理解为 dp table
            // 定义：distTo[i] 的值就是节点 start 到达节点 i 的最短路径权重
            int[] distTo = new int[V];
            // 求最小值，所以 dp table 初始化为正无穷
            Arrays.fill(distTo, Integer.MAX_VALUE);
            // base case，start 到 start 的最短距离就是 0
            distTo[start] = 0;

            // 优先级队列，distFromStart 较小的排在前面
            Queue<State> pq = new PriorityQueue<>((a, b) -> {
                return a.distFromStart - b.distFromStart;
            });

            // 从起点 start 开始进行 BFS
            pq.offer(new State(start, 0));

            while (!pq.isEmpty()) {
                State curState = pq.poll();
                int curNodeID = curState.id;
                int curDistFromStart = curState.distFromStart;

                if (curDistFromStart > distTo[curNodeID]) {
                    // 已经有一条更短的路径到达 curNode 节点了
                    continue;
                }
                // 将 curNode 的相邻节点装入队列
                for (int nextNodeID : adj(curNodeID)) {
                    // 看看从 curNode 达到 nextNode 的距离是否会更短
                    int distToNextNode = distTo[curNodeID] + weight(curNodeID, nextNodeID);
                    if (distTo[nextNodeID] > distToNextNode) {
                        // 更新 dp table
                        distTo[nextNodeID] = distToNextNode;
                        // 将这个节点以及距离放入队列
                        pq.offer(new State(nextNodeID, distToNextNode));
                    }
                }
            }
            return distTo;
        }

        ```
        * The claim for BFS is that the first time a node is discovered during the traversal, that distance from the source would give us the shortest path. The same cannot be said for a weighted graph. For a weighted graph, there is no correlation between the number of edges composing the path and the actual length of the path which is composed of the weights of all the edges in it. Thus, we cannot employ breadth-first search for weighted graphs.
        * In unweighted graphs, when we reached a node from a different path, we were sure that the first time should have the shortest path. In weighted graphs, that’s not always true. If we reached the node with a shorter path, we must update its distance and add it to the queue. **This means that the same node could be added multiple times**.
        * Prim’s adds next the non-tree vertex that is closest to the tree; Dijkstra’s adds next the non-tree vertex that is closest to the source.
        * The algorithm works without using `visited` set to record the nodes that have been visited, because the nodes that we should skip already have the shortest distance, so they don't satisfy the condition to be pushed into the priority queue again. 
            * Note, if you use 'visited' set while looping through the neighbors, this is wrong. Because not all neighbors will have the shortest distance at this time, and they may need to be revisited to update their distances to the source.
        * Complexity: `O(VlogV +E)`, practically linear time, mostly dominated by `E`. 
            * Note, the complexity doesn't depend on the dynamic range of the weights at all.
            * BFS and DFS aren't directly applicable to the shortest path problem for graph with weights because of the dynamic weights.
            * Operations in Dijkstra
                * `O(V)`: insertion into priority queue (ADT)
                    * array: `O(V)`
                    * min-heap: `O(logV)`
                * `O(V)`: extract-min from priority queue 
                    * array: `O(V)`
                    * min-heap: `O(logV)` extract min
                * `O(E)`: relax (decrease key) operation on each vertex            
                    * array: `O(1)`
                    * min-heap: `O(logV)` for relaxation
                * Total complexity:
                    * Array: `O(V^2+E)=O(V^2)`
                    * Heap: `O(VlogV + ElogV)` (Fibonacci heap can remove the `logV` in second term)
        * Need keep the predecessors leading up to current vertex
        * Note that according to the algorithm, once a node has been processed i.e., once a node is popped from the min-heap, we never consider that node again in some other node's neighbors i.e., we never add it again to the heap down the line. 
        * Template: #shortest distance from `(0,0)` to `(n-1,n-1)`, `grid[i][j]` is cost?
        ```
        seen = set([(0,0)])
        min_heap = [(grid[0][0], 0,0)]
        n = len(grid)        
        while min_heap:
            v, i, j = heappop(min_heap)                        
            if (i,j) == (n-1, n-1):
                return v            
            for di, dj in [[1,0], [-1, 0], [0,1], [0,-1]]:
                ni, nj = i+di, j+dj
                if 0<=ni<n and 0<=nj<n and (ni,nj) not in seen:                    
                    seen.add((ni,nj))
                    d = max(v, grid[ni][nj])
                    heappush(min_heap, (d, ni,nj))
        ```
        * Variations
            * If there are additional conditions to satisfy, e.g. maximum number of edges, in addition to the minimum total distance, we should **relax** both at the same time.
    5. Topological Sort for DAG (dynamic programming)
        * **Whenenver you have DAG, try topological sort the DAG**, path from `u` to `v` implies that `u` is before `v` in ordering.
            * There's no cycles
            * You can start from any vertex
        * One pass (left to right) over the vertices in topologically sorted order,  relax each edge that leaves each vertex we are processing right now.
        * `O(V+E)`
        * This works with negative weights, but not with negative cycle
        * Problems:
            * [LC64. Minimum Path Sum][LC64. Minimum Path Sum]
    6. Example: [The Maze II ][LC505 The Maze II], [The Maze III][LC499 The Maze III]

7. The Longest Path in the Graph (Diameter of Graph)
    1. We can loop through each node, and find the longest path from the node to any other node. It may take too long. It turns out that we don't have to run it for each node.
        * First of all, we will use the concepts of **leaf** nodes and **parent** nodes as in a Tree data structure. For a parent node, if we could obtain two longest distances (denoted as t1 and t2) starting from this parent node to any of its descendant leaf nodes, then the longest path that traverse this parent node would be `t1 + t2`.
        * Since any node in the graph has the potential to be part of the path that forms the diameter of the graph, we can iterate through each node to obtain all the longest paths as we defined shortly before. The diameter of the graph would be the maximum among all the longest paths that traverse each node.

    2. According to the definition, the problem could be solved if we could identify the two nodes that have the longest distance among all. Let us refer to these two nodes as the **extreme peripheral** nodes.
        * First of all, we assert that starting from any node in the graph, if we run a BFS traversal, the last node that we visit would be **one of the extreme nodes**. 
            * An intuition that supports the above assertion is that as an extreme peripheral node, it should be the one that is far away from any of the other nodes in the graph. Given any node, the longest distance that starts from this node must end with one of the extreme peripheral nodes.
        * Once we identify one of the extreme peripheral nodes, we then could apply again the BFS traversal. But this time, we would start from the identified extreme peripheral node. At the end of the second BFS traversal, we would land on another extreme peripheral node. The distance that we traverse would be the diameter of the graph, according to the definition.
        * We can use BFS to find one extreme first, then call BFS again to find the distance from this extreme to the other.
    3. Another concept that is closely related to the concept of diameter is called "centroid". Intuitively, the centroid nodes are the ones that are situated in the center of a graph. More precisely, the distance from the centroid to other nodes in the graph should be overall minimal, which is the opposite of the extreme peripheral nodes that we defined before.
        * Indeed, if we could identify the centroid of a graph, then the distance from this centroid to any of its extreme peripheral nodes would be half of the diameter of the graph.
        * For the tree-alike graph, the number of centroids is no more than 2.
            * There would be **either one or two centroids** in a tree-alike graph, and one can find a detailed proof in the solution of the [Minimum Height Tree][LC310 Minimum Height Tree].
            * For any of the cases that have more than 2 centroids, they must form a cycle among the centroids
        * If there is only one centroid, the diameter would be exactly twice of the distance between the centroid and any of extreme peripheral nodes.
        * If there are two centroids, the diameter would be one plus the distance between a pair of centroid and extreme peripheral node, by taking into account the distance between the two centroids.
        * In order to identify the centroids, we could apply the **topological sorting algorithm** (BFS) here. The main idea is that we start from the peripheral nodes (which are also known as leaf nodes), then we trim the nodes off layer by layer, as if we are peel an "onion" till we reach its core (i.e. centroids).
            * The above algorithm resembles the topological sorting algorithm which generates the order of objects based on their dependencies. For instance, in the scenario of course scheduling, the courses that have the least dependency would appear first in the order.
            * N.B. We have to actually remove the edges layer by layer, otherwise we don't know when to stop. So no `visited` is needed here. 
        * In certain sense, we could also consider the above topological sorting as a sort of BFS Traversal, where we traverse the graph from the outer most layer of the graph (i.e. leaf nodes), level by level, into its inner most layer (i.e. centroids). `O(V)`. N.B. As each node is connected with other nodes without cycle, there are only `O(N-1)` edges.
        ```
        dq = deque()
        for node, neighbors in graph.items():
            if len(neighbors) == 1:
                dq.append(node)
        
        remaining = n
        while remaining > 2: #Can't use the number of nodes on deque to stop
            sz = len(dq)
            for _ in range(sz):
                current = dq.popleft()                   
                for neighbor in list(graph[current]):        
                    #Remove edge once we use it                            
                    graph[current].remove(neighbor)
                    graph[neighbor].remove(current)
                    if len(graph[neighbor]) == 1: #Add nodes with only 1 edge
                        dq.append(neighbor)
            remaining -= sz                        
        return dq
        ```        

8. Traversal Graph
    0. In graph theory, an Eulerian trail (or `Eulerian path`) is a trail in a finite graph that visits **every edge** exactly once (allowing for revisiting vertices).
        * Similarly, an Eulerian circuit or Eulerian cycle is an Eulerian trail that starts and ends on the same vertex.
    1. Deal with duplicate edges, e.g. multiple edges with the same start and end nodes
        * When create the adjacency list, add an extra index for each pair of node
        * You can make the visited set be a dictionary, each start node maps to a destination's index
        * Or you just remove the node from the graph once visited
    2. Keep a list of ordered nodes in the adjacency list
        * sort after insert all nodes
        * use priority queue
    3. Solutions
        * Backtrack
        * Hierholzer's Algorithm
            * The basic idea of Hierholzer's algorithm is the stepwise construction of the Eulerian cycle by connecting disjunctive circles.
            * It starts with a random node and then follows an arbitrary unvisited edge to a neighbor. This step is repeated until one returns to the starting node. This yields a first circle in the graph.
            * If this circle covers all nodes it is an Eulerian cycle and the algorithm is finished. Otherwise, one chooses another node among the cycles' nodes with unvisited edges and constructs another circle, called subtour.
            * To find the Eulerian path, inspired from the original Hierzolher's algorithm, we simply change one condition of loop, rather than stopping at the starting point, we stop at the vertex where we do not have any unvisited edges.
            * The first vertex that we got stuck at would be the end point of our Eulerian path. So if we follow all the stuck points backwards, we could reconstruct the Eulerian path at the end.
            * Actually, we could consider the algorithm as the **postorder DFS** in a directed graph, from a fixed starting point.

9. Topological Sort
    * Scheduling problems: Precedence-constrained scheduling
        * Given a digraph, put the vertices in order such that all its directed edges point from a vertex earlier in the order to a vertex later in the order (or report that doing so is not possible)
        * It is based on the idea that depth-first search visits each vertex exactly once. If we save the vertex given as argument to the recursive `dfs()` in a data structure, then iterate through that data structure, we see all the graph vertices, in order determined by the nature of the data structure and by whether we do the save before or after the recursive calls. Three vertex orderings are of interest in typical applications:
            * Preorder: Put the vertex on a queue before the recursive calls
            * Postorder : Put the vertex on a queue after the recursive calls
            * Reverse postorder (Topologic sort) : Put the vertex on a stack after the recursive calls (`V+E`)
        * It usually applies on a DAG, so there's no back edge (no cycle). 
    * Run DFS output reverse of finishing times (the time after we finished processing all of its adjcent vertices) of vertices. 后序遍历的这一特点很重要，之所以拓扑排序的基础是后序遍历，是因为一个任务必须等到它依赖的所有任务都完成之后才能开始开始执行。
        * We make use of a temporary stack. We do the traversals in the same manner as in DFS, but we don’t print the current node immediately. Instead, for the current node we do as follows:
            * Recursively call topological sorting for all the nodes adjacent to the current node.
            * Push the current node onto a stack (when we exhausted its neighbors).
            * Repeat the above process till all the nodes have been considered at least once.
            * Print the contents of the stack (just pop stack one by one).
        * Note that a vertex is pushed to stack only when all of its adjacent(dependent) vertices (and their adjacent(dependent) vertices and so on) are already in stack. Thus, we obtain the correct ordering of the vertices.
        * This handles cycles, duplicate edges (You can also use `defaultdict(set)` to represent adjacency list to remove duplicates), node pointing to itself, multiple clusters cases

        ```
        colors = {v:0 for v in range(numCourses)}
        has_cycle = False
        res = []
        def dfs(start):
            nonlocal has_cycle       
            if has_cycle:
                return            
            colors[start] = 1
            for neighbor in course_dep[start]:
                if colors[neighbor] == 0:
                    dfs(neighbor)
                elif colors[neighbor] == 1: #check cycle
                    has_cycle = True
                    return
            colors[start] = 2
            res.append(start) #reverse post order traversal
            
        for node in range(numCourses):
            if colors[node] == 0: #N.B. We don't have to start from node with zero in-degrees
                dfs(node)

        return res[::-1] if not has_cycle else []        
        ```
    * BFS (Kahn's Algorithm): 
        * Start from nodes with in-degree of `0`, and move to inside nodes, remove current edge along the way, each time we find the new nodes that with updated in-degree of `0`.
        * This handles duplicate edges (You can also use `defaultdict(set)` to represent adjacency list to remove duplicates), cycle, 
        * To handle the case where a node points to itself, don't add it in the graph adjacency list
        * N.B. There can be more than one valid orders, here we just return one of them
        * `O(V+E)` in both time and space.
        ```
        course_dep = defaultdict(list)
        degrees = defaultdict(int)
        for (course, prereq) in prerequisites:
            course_dep[prereq].append(course)
            degrees[course] += 1
        
        # Handles the case of lonely nodes, multiple clusters
        zero_nodes = set(range(numCourses)) - set(degrees.keys())  
        res = []
        pq = deque(zero_nodes)                
        while pq:
            current = pq.popleft()
            res.append(current)
            for neighbor in course_dep[current]:                
                degrees[neighbor] -= 1
                if degrees[neighbor] == 0:
                    pq.append(neighbor)
        
        # If there's a cycle, it's impossible to visit all nodes, as the entry of cycle won't ever have degree of 0
        return res if len(res) == numCourses else []     #This detects cycle at the same time        
        ```
    3. Problems
        * [LC269. Alien Dictionary][LC269. Alien Dictionary]

10. [名流问题][众里寻他千百度：名流问题]
    1. 名人节点的出度为 0，入度为 n - 1. 于名人问题，显然会经常需要判断两个人之间是否认识，也就是两个节点是否相邻，所以我们可以用邻接矩阵来表示人和人之间的社交关系。
    2. 因为「名人」的定义保证了「名人」的唯一性，所以我们可以利用排除法，先排除那些显然不是「名人」的人，从而避免 for 循环的嵌套，降低时间复杂度。我们可以不断从候选人中选两个出来，然后排除掉一个，直到最后只剩下一个候选人，这时候再使用一个 for 循环判断这个候选人是否是货真价实的「名人」。
    3. [LC277. Find the Celebrity][LC277. Find the Celebrity]


11. Variations       
    1. If needed, think about removing nodes that have been visited
    2. Problems
        * [LC126. Word Ladder II][LC126. Word Ladder II]



11. Cases to consider when dealing with Graph
    1. The graph is not connected, i.e. it's partitioned into clusters, where one node in a cluster is not connected to another node in a different cluster.
       * A node not connecting with any other nodes
    2. The cluster may have cycle, e.g. `a->b->a` or `a->b->c->b`    
    3. A node points to itself, `a->a`
    4. Duplicate edges between two nodes, the same edge appears twice, e.g. `a->b` and `a->b`
    5. Two nodes point to the same node
       * `a->b` and `c->b`
       * `a->b->c` and `a->c` at the same time
    6. One node points to multiple nodes: `a->b`, `a->c`





[LeetCode Link]: https://leetcode.com/problems/making-a-large-island/
[LC505 The Maze II]: https://leetcode.com/problems/the-maze-ii/
[LC1197 Minimum Knight Moves]: https://leetcode.com/problems/minimum-knight-moves/
[LC310 Minimum Height Tree]: https://leetcode.com/problems/minimum-height-trees/solution/
[why-dfs-and-not-bfs-for-finding-cycle-in-graphs]: https://stackoverflow.com/questions/2869647/why-dfs-and-not-bfs-for-finding-cycle-in-graphs
[LC499 The Maze III]: https://leetcode.com/problems/the-maze-iii/
[LC1168 Optimize Water Distribution in a Village]: https://leetcode.com/problems/optimize-water-distribution-in-a-village/
[LC64. Minimum Path Sum]: https://leetcode.com/problems/minimum-path-sum/
[LC200. Number of Islands]: https://leetcode.com/problems/number-of-islands/
[LC126. Word Ladder II]: https://leetcode.com/problems/word-ladder-ii/
[LC269. Alien Dictionary]: https://leetcode.com/problems/alien-dictionary/
[LC785. Is Graph Bipartite?]: https://leetcode.com/problems/is-graph-bipartite/
[DIJKSTRA 算法模板]: https://labuladong.github.io/algo/2/20/55/
[图论基础]: https://labuladong.github.io/algo/2/20/47/
[KRUSKAL 最小生成树算法]: https://labuladong.github.io/algo/2/20/51/
[众里寻他千百度：名流问题]: https://labuladong.github.io/algo/2/20/53/
[LC277. Find the Celebrity]: https://leetcode.com/problems/find-the-celebrity/
[我写了一个模板，把 DIJKSTRA 算法变成了默写题]: https://labuladong.github.io/algo/2/20/54/