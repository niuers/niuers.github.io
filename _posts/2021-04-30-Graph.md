---
title: "Graph"
date: 2021-04-30
categories:
  - blog
tags:
  - algorithm
  - graph
  - summary
---

1. Definitions:
    1. A graph is connected if there is a path from every vertex to every other vertex in the graph.
    2. Acyclic graph: a graph with no cycles.
    3. Tree is an acyclic connected graph.    
    4. Spanning Tree: A spanning tree of a graph is a connected subgraph with no cycles that include all vertices.
    5. For simple graph (at most one edge between any pair of vertices), the asympotic relationship between `V` and `E` is: `E=O(V^2)`, the upper bound is achieved in a complete graph.
    6. Adjacency-lists (linked list) data structure is used to represent the edges for vertex.
    7. A directed acyclic graph (DAG) is a digraph with no directed cycles.


2. Directed Graph (Digraph)
    1. Reverse a digraph
    2. Multiple-source reachability. Given a digraph and a set of source vertices, support queries of the form "Is there a directed path from any vertex in the set to a given target vertex v?" (Mark-and-sweep garbage collection)
        * DFS
    3. Scheduling problems: Precedence-constrained scheduling (Topological sort)
        * Given a digraph, put the vertices in order such that all its directed edges point from a vertex earlier in the order to a vertex later in the order (or report that doing so is not possible)
        * It is based on the idea that depth-first search visits each vertex exactly once. If we save the vertex given as argument to the recursive `dfs()` in a data structure, then iterate through that data structure, we see all the graph vertices, in order determined by the nature of the data structure and by whether we do the save before or after the recursive calls. Three vertex orderings are of interest in typical applications:
            * Preorder: Put the vertex on a queue before the recursive calls
            * Postorder : Put the vertex on a queue after the recursive calls
            * Reverse postorder (Topologic sort) : Put the vertex on a stack after the recursive calls (`V+E`)
    4. Cycles in digraphs (Directed Cycle Detection)
        * Does a given digraph have a directed cycle? If so, find the vertices on some such cycle, in order from some vertex back to itself. i.e. Is it a DAG? 
        * Backtracking: put a node in the visited path, check, then remove the node from visited path.
            * Postorder DFS: It uses a boolean array `onStack[]` to keep track of the vertices for which the recursive call has not completed. When it finds a destination vertex `w`, from edge `v->w`, is on the stack, it has discovered a directed cycle, which it can recover by following `edgeTo[]` links. `O(E+V)`, space `O(E+V)`.
            * Worst case: all nodes are chained in a line, and we need check for each node
            ```
            def dfs(course, stack, visited):    
                stack.append(course)
                visited.add(course)
                for neighbor in graph[course]:
                    if neighbor not in visited:
                        if not dfs(neighbor, stack, visited)
                            return False
                    elif neighbor in stack:
                        return False
                stack.pop()
                return True
                        
            visited = set()
            for course in range(numCourses):            
                if course not in visited and not dfs(course, [], visited):
                    return False        
            return True
        ```        
        * The node-coloring variant of the algorithm, The idea is to do DFS of a given graph and while doing traversal, assign one of the below three colors to every vertex.
            * WHITE ~ Vertex is not processed yet. Initially, all vertices are WHITE.
            * GRAY ~ Vertex is being processed (DFS for this vertex has started, but not finished which means that all descendants (in DFS tree) of this vertex are not processed yet (or this vertex is in the function call stack).
            * BLACK ~ Vertex and all its descendants are processed.
            * While doing DFS, if an edge is encountered from current vertex to a GRAY vertex, then this edge is a back edge and hence there is a cycle. A GRAY node represents a node whose processing is still ongoing. Thus, if a descendent eventually leads back to a node whose processing is ongoing, it ends up creating a cycle in the directed graph and we call the edge leading back to a GRAY node as a backward edge.
            * There are three possibilities for the color of this current node.
                * If it is BLACK, do nothing; we've already processed it.
                * If it is WHITE, then mark it as GRAY since we are starting the processing of the graph rooted at this node.
                * Otherwise, if it is GRAY, then return false as we have discovered a backward edge, and hence a cycle.
            ```
            def dfs(course, nodes):                
                nodes[course] = 1
                for neighbor in graph[course]:
                    if nodes[neighbor] == 1 or (nodes[neighbor] == 0 and not dfs(neighbor, nodes)):
                        return False
                nodes[course] = 2
                return True
                        
            nodes = {i:0 for i in range(numCourses)} #white: 0, gray: 1, black: 2
            for course in range(numCourses):            
                if nodes[course] == 0 and not dfs(course, nodes):
                    return False        
            return True            
            ```
        * Why not Breadth-First Search?
            * From [this Stack Overflow answer][why-dfs-and-not-bfs-for-finding-cycle-in-graphs]:
            * A BFS could be reasonable if the graph is undirected, where each cross edge defines a cycle (edge going from a node to an already visited node). If the cross edge is `{v1, v2}`, and the root (in the BFS tree) that contains those nodes is `r`, then the cycle is `r ~ v1 - v2 ~ r` (`~` is a path, `-` a single edge), which can be reported almost as easily as in DFS.
            * The only reason to use a BFS would be if you know your (undirected) graph is going to have long paths and small path cover (in other words, deep and narrow). In that case, BFS would require proportionally less memory for its queue than DFS' stack (both still linear of course).
            * In all other cases, DFS is clearly the winner.


2. Depth-First Search (DFS)
    1. Recursive:
        * Complexity: DFS marks all the vertices connected to a given source in time proportional to the sum of their degrees (`2E`). 
            * To run DFS on all vertices of a graph (so we need visit each vertex once at least), the complexity is `V+E`.
        * We encounter each edge in the graph twice (once at each of its vertices)
        * Careful not to repeat vertices
        * N.B. It's possible that we'll get stackoverflow when using recursion method. If that can be the case, use the stack version instead.
        ```
        def DepthFirstSearch(adjacency_list, s: 'Node'):
            marked = {}
            dfs(adjacency_list, s)
        
        def dfs(adjacency_list, start_node: 'Node', marked):
            marked[start_node] = True
            count += 1 #some action            
            for w in adjacency_list[start_node].neighbors:
                if not marked[w]:
                    dfs(adjacency_list, w, marked)   
        
        # A 2D matrix example to find the islands marked by '1'. If the matrix is MxN, then for this graph there can be a max of MxN vertices, and max of M*(N-1)+N*(M-1) edges. 

        m, n = len(grid), len(grid[0])
        visited = set() 
        # The max length of system stack can be O(MxN) if every node is '1'.   
        def dfs(x, y):            
            visited.add((x, y))
            for (dx, dy) in [[0,1], [0, -1], [1, 0], [-1, 0]]:
                nx, ny = x+dx, y+dy
                if nx >= 0 and nx < m and ny >= 0 and ny < n and grid[nx][ny] == '1' and (nx, ny) not in visited:                    
                    dfs(nx, ny)        

        res = 0
        for i in range(m):
            for j in range(n):
                if grid[i][j] == '0' or (i,j) in visited:
                    continue                
                dfs(i, j)
                res += 1
        
        return res        
        ```
        * Problems
            * [LC200. Number of Islands][LC200. Number of Islands]

    2. Uses LIFO stack: 
    ```
    stack = [start_node]
    visited = set([start_node])    
    while stack:
        current = stack.pop()        
        # N.B. Avoid putting the same node onto stack twice. so we need add a node into the visited set  # right before we add it into the stack
        # It won't work if put visited.add(current) here. For example, If node A, B, C are in the stack 
        # and `current = A`, if B is A's neighbor, since B is not in the visited set, we are going to add # B into the stack, which is wrong since B is already on stack. Think about a graph of 3 nodes    # with triangle shape.

        for neighbor in current.neighbors:
            if not neighbor in visited:
                visited.add(neighbor)
                stack.append(neighbor)    

        # A 2D Matrix Example:
        m, n = len(grid), len(grid[0])
        visited = set()

        # What's the maximum length of stack here ? 
        def dfs(x, y):            
            stack = [(x,y)]
            visited.add((x,y))
            while stack:
                cx, cy = stack.pop()                
                for (dx, dy) in [[0,1], [0, -1], [1, 0], [-1, 0]]:
                    nx, ny = cx+dx, cy+dy
                    if nx >= 0 and nx < m and ny >= 0 and ny < n and grid[nx][ny] == '1' and (nx, ny) not in visited:                    
                        visited.add((nx,ny))
                        stack.append((nx,ny))
                
        res = 0
        for i in range(m):
            for j in range(n):
                if grid[i][j] == '0' or (i,j) in visited:
                    continue                
                dfs(i, j)
                res += 1
        
        return res                    
    ```
    3. Single-source connectivity: Are two given vertices connected (path detection) ? and How many connected components does the graph have ?
        * DFS: `V+E`
        * Union-Find: however it can't find the path
            * It's an online algorithm: we can check whether two vertices are connected in near-constant time at any point, even while adding edges
    4. Single-source paths (Finding Paths). Given a graph and a source vertex s, support queries of the form "Is there a path from s to a given target vertex v? If so, find such a path".
        * DFS
            * It maintains a vertex-indexed array `edgeTo[]` or `parent[]` such that `edgeTo[w] = v` means that `v-w` was the edge used to access `w` for the first time. The `edgeTo[]` array is a parent-link representation of a tree rooted at `s` that contains all the vertices connected to `s`. Instead of just keeping track of the path from the current vertex back to the start
            * The paths discovered by depth-first search depend not just on the graph, but also on the representation and the nature of the recursion.    
    4. Two-colorability: Can the vertices of a given graph be assigned one of two colors in such a way that no edge connects vertices of the same color ? which is equivalent to this question: Is the graph bipartite ?
    5. Edge Classification
        * Tree edges: (parent pointer in DFS), the edges that are used in DFS to visit new vertex. They form a tree (and forest).
        * Forward edges
        * Backward edges: if a vertex is already on the stack, then current edge is back edge
        * Cross edges
        * In an undirected graph, only tree edges and backward edges can exist. There are no forward/cross edges.
    6. Cycle detection        
        * A graph `G` has a cycle if and only if the depth-first search of `G` has a back edge.
    7. Topological Sort
        * It usually applies on a DAG, so there's no back edge (no cycle). 
        * Run DFS output reverse of finishing times (the time after we finished processing all of its adjcent vertices) of vertices
            * We make use of a temporary stack. We do the traversals in the same manner as in DFS, but we donâ€™t print the current node immediately. Instead, for the current node we do as follows:
                * Recursively call topological sorting for all the nodes adjacent to the current node.
                * Push the current node onto a stack (when we exhausted its neighbors).
                * Repeat the above process till all the nodes have been considered at least once.
                * Print the contents of the stack (just pop stack one by one).
            * Note that a vertex is pushed to stack only when all of its adjacent(dependent) vertices (and their adjacent(dependent) vertices and so on) are already in stack. Thus, we obtain the correct ordering of the vertices.
        ```
        colors = {v:0 for v in range(numCourses)}
        has_cycle = False
        res = []
        def dfs(start):
            nonlocal has_cycle            
            if has_cycle:
                return            
            colors[start] = 1
            for neighbor in course_dep[start]:
                if colors[neighbor] == 0:
                    dfs(neighbor)
                elif colors[neighbor] == 1:
                    has_cycle = True
                    return
            colors[start] = 2
            res.append(start)
            
        for node in range(numCourses):
            if colors[node] == 0:
                dfs(node)
        
        return res[::-1] if not has_cycle else []        
        ```
        * BFS (Kahn's Algorithm): 
            * Start from nodes with in-degree of `0`, and move to inside nodes, remove current edge along the way, each time we find the new nodes that with updated in-degree of `0`.
            * `O(V+E)` in both time and space.
            ```
            course_dep = defaultdict(list)
            degrees = defaultdict(int)
            for (course, prereq) in prerequisites:
                course_dep[prereq].append(course)
                degrees[course] += 1
                            
            zero_nodes = set(range(numCourses)) - set(degrees.keys())        
            res = []
            pq = deque(zero_nodes)                
            while pq:
                current = pq.popleft()
                res.append(current)
                for neighbor in course_dep[current]:                
                    degrees[neighbor] -= 1
                    if degrees[neighbor] == 0:
                        pq.append(neighbor)
            return res if len(res) == numCourses else []     #This detects cycle at the same time        
            ```

3. Breadth-First-Search (BFS)
    1. The goal is to visit all the nodes reachable from a given source node `s`
        1. It uses a FIFO queue: 
        ```
        # Space complexity : O(min(M, N)) because in worst case where the grid is filled with lands, the size of queue can grow up to min(M,N). N.B. It's not max(M,n) here.

        m, n = len(grid), len(grid[0])
        visited = set()
        def bfs(x, y):            
            dq = deque([(x,y)])
            visited.add((x,y))
            while dq:
                sz = len(dq)
                for i in range(sz):
                    cx, cy = dq.popleft()                
                    for (dx, dy) in [[0,1], [0, -1], [1, 0], [-1, 0]]:
                        nx, ny = cx+dx, cy+dy
                        if nx >= 0 and nx < m and ny >= 0 and ny < n and grid[nx][ny] == '1' and (nx, ny) not in visited:                    
                            visited.add((nx,ny))
                            dq.append((nx,ny))
                
        res = 0
        for i in range(m):
            for j in range(n):
                if grid[i][j] == '0' or (i,j) in visited:
                    continue                
                bfs(i, j)
                res += 1
        
        return res        
        ```
        2. BFS takes time proportional to `V+E` in the worst case. You only check each edge once (`|E|` in directed graph) or twice (`2|E|` in undirected graph), and a vertex enters the queue only once. 
        3. Need to be carful to avoid duplicates by using a 'visited' set
            * Be careful on where to add the vertex into the 'visited' set though. 
            * Shall we add a vertex into the 'visited' set when we pop it out of the queue?
            * Shall we add a neighbor of a vertex into the 'visited' set before appending it onto the queue while we loop through a vertex's neighbors? 
            * The latter is more efficient as we avoid adding many more vertices into the queue. If we use the former, the distance between each visited vertex can be much larger. See [Minimum Knight Moves][LC1197 Minimum Knight Moves].
        4. "Pruning" can be used to improve the efficency of BFS. e.g. We can use bidirectional BFS instead of unidirectional BFS. or We can remove unwanted vertices from the queue.
    2. **Single-source shortest paths**. Given a graph and a source vertex `s`, support queries of the form "Is there a path from s to a given target vertex v? If so, find a shortest such path (one with a minimal number of edges, not necessarily the only one)"
        * BFS: It also maintains `edgeTo[]` or `parent[]` to save the paths
    3. Dijkstraâ€™s algorithm solves the **single-source shortest-paths problem in edge-weighted digraphs with nonnegative weights**.
        * All-pairs shortest paths. Given an edge-weighted digraph, support queries of the form Given a source vertex s and a target vertex t, is there a path from s to t? If so, find a shortest such path (one whose total weight is minimal).

4. Minimum Spanning Tree (MST) of Edge-Weighted Undirected Graph
    0. A minimum spanning tree of an edge-weighted graph is a spanning tree whose weight (the sum of the weights of its edges) is no larger than the weight of any other spanning tree.
    1. Adding an edge that connects two vertices in a tree creates a unique cycle
    2. Removing an edge from a tree breaks it into two separate subtrees
    3. Cut Property: A cut of a graph is a partition of its vertices into two nonempty disjoint sets. A crossing edge of a cut is an edge that connects a vertex in one set with a vertex in the other.
        * Given any cut in an edge-weighted graph, the crossing edge of minimum weight is in the MST of the graph.
        * Note that there is no requirement that the minimal edge be the only MST edge connecting the two sets; indeed, for typical cuts there are several MST edges that connect a vertex in one set with a vertex in the other
    4. The edge weights are all different. If edges can have equal weights, the minimum spanning tree may not be unique.
    5. Greedy Algorithm
        1. The following method colors black all edges in the the MST of any connected edge-weighted graph with `V` vertices: starting with all edges colored gray, find a cut with no black edges, color its minimum-weight edge black, and continue until `V-1` edges have been colored black.
        2. Prim's Algorithm
            1. Use priority queue to hold crossing edges and find the cut with minimum weight
            2. We can use eager algorithm, we maintain on the priority queue just one edge for each non-tree vertex `w`: the shortest edge that connects it to the tree.
            3. Time complexity: `O(ElogE)` (lazy version), `O(ElogV)` (eager version)
            4. The process of building MST consists of a loop with the following substeps:
                * Each iteration, we pop an element from the heap. This element contains a vertex along with the cost that is associated with the edge that connecting the vertex to the tree. The vertex is chosen if it is not already in the tree. We know that the cost of this vertex is minimal among all choices because it was popped from the heap.
                * Once the vertex is added, we then examine its neighboring vertices. Specifically, we add these vertices along with their edges into the heap as the candidates for the next round of selection.
                * The loop terminates when we have added all the vertices from the graph into the MST.
                * NB. You can use `defaultdict(list)` to store both the adjacency list and edge weights like this: `graph[n0] = [(w1, n1), (w3, n3)]`.
                * NB. It's possible that even for undirectional graph, there can be two edges between a pair of nodes, e.g. `weight(n5, n6) = 7` and `weight(n6, n5) = 8`. If such case is possible, we should store the edge weights in both directions.

        3. Kruskal's Algorithm
            1. It processes the edges in order of their weight values (smallest to largest), taking for the MST (coloring black) each edge that does not form a cycle with edges previously added, stopping after adding `V-1` edges have been taken.
                * A more concise criteria to determine whether we should add a new edge in Kruskal's algorithm is that whether both ends of the edge belong to the same component (group).
            2. It use a priority queue to consider the edges in order by weight, a **union-find** data structure to identify those that cause cycles, and a queue to collect the MST edges.
            3. Space complexity: `O(E)`, Time complexity: `O(ElogE)`
    6. Related Problems
        1. [Optimize Water Distribution in a Village][LC1168 Optimize Water Distribution in a Village]

6. The shortest path (i.e. path with the minimum total weights) from a source
    1. Example: [The Maze II ][LC505 The Maze II], [The Maze III][LC499 The Maze III]
    2. Optimal Substructure
        * Subpaths of a shortest path are shortest paths
    3. Algorithms
        * Dijkstra Algorithm
        * Bellmen-Ford Algorithm
            * Works on edge with both positive and negative weights
            * `O(VE)`
            * Negative weight cycle: need detect and mark the related vertices with infinity cost
        * General Structure of the shortest path problem (no negative cycles)
            * Initialize: set source distance `d[s]=0`, and for each vertex `u`, set distance `d[u]`(The length of the current shortest path from source `s` to `u`) to `infinity`, and predecessor `p[u]` (The predecessor of `u` in the shortest path from `s` to `u`) as `None`
            * Repeat: select some edge `(u,v)` in some way (Algorithm specific)
            * **Relaxation** of `edge(u,v)` (from `u` to `v`): 
            ```
            if d[v] > d[u] + w(u,v):
                d[v] = d[u] + w(u,v)
                p[v] = u #predecessor
            ```
            * Until all edges have `d[v] <= d[u] + w(u,v)` (brutal force)
            * This algorithm depends on the ordering of vertices we choose which can be exponentially time complexity
    4. Dijkstra Algorithm (Greedy)
        * It assumes nonnegative weights  
        * It works with graph with cycles
        * The algorithm works without using 'visited' set to record the nodes that have been visited, because the nodes that we should skip already have the shortest distance, so they don't satisfy the condition to be pushed into the priority queue again. 
            * Note, if you use 'visited' set while looping through the neighbors, this is wrong. Because not all neighbors will have the shortest distance at this time, and they may need to be revisited to update their distances to the source.
        * Complexity: `O(VlogV +E)`, practically linear time, mostly dominated by `E`. 
            * Note, the complexity doesn't depend on the dynamic range of the weights at all.
            * BFS and DFS aren't directly applicable to the shortest path problem for graph with weights because of the dynamic weights.
            * Operations in Dijkstra
                * `O(V)`: insertion into priority queue (ADT)
                    * array: `O(V)`
                    * min-heap: `O(logV)`
                * `O(V)`: extract-min from priority queue 
                    * array: `O(V)`
                    * min-heap: `O(logV)` extract min
                * `O(E)`: relax (decrease key) operation on each vertex            
                    * array: `O(1)`
                    * min-heap: `O(logV)` for relaxation
                * Total complexity:
                    * Array: `O(V^2+E)=O(V^2)`
                    * Heap: `O(VlogV + ElogV)` (Fibonacci heap can remove the `logV` in second term)
        * Need keep the predecessors leading up to current vertex

    5. The Shortest Path Algorithm for DAG (dynamic programming)
        * **Whenenver you have DAG, try topological sort the DAG**, path from `u` to `v` implies that `u` is before `v` in ordering.
            * There's no cycles
            * You can start from any vertex
        * One pass (left to right) over the vertices in topologically sorted order,  relax each edge that leaves each vertex we are processing right now.
        * `O(V+E)`
        * This works with negative weights, but not with negative cycle
        * Problems:
            * [LC64. Minimum Path Sum][LC64. Minimum Path Sum]


7. The Longest Path in the Graph (Diameter of Graph)
    1. We can loop through each node, and find the longest path from the node to any other node. It may take too long. It turns out that we don't have to run it for each node.
        * First of all, we will use the concepts of **leaf** nodes and **parent** nodes as in a Tree data structure. For a parent node, if we could obtain two longest distances (denoted as t1 and t2) starting from this parent node to any of its descendant leaf nodes, then the longest path that traverse this parent node would be `t1 + t2`.
        * Since any node in the graph has the potential to be part of the path that forms the diameter of the graph, we can iterate through each node to obtain all the longest paths as we defined shortly before. The diameter of the graph would be the maximum among all the longest paths that traverse each node.
        *

    2. According to the definition, the problem could be solved if we could identify the two nodes that have the longest distance among all. Let us refer to these two nodes as the **extreme peripheral** nodes.
        * First of all, we assert that starting from any node in the graph, if we run a BFS traversal, the last node that we visit would be one of the extreme nodes. An intuition that supports the above assertion is that as an extreme peripheral node, it should be the one that is far away from any of the other nodes in the graph. Given any node, the longest distance that starts from this node must end with one of the extreme peripheral nodes.
        * Once we identify one of the extreme peripheral nodes, we then could apply again the BFS traversal. But this time, we would start from the identified extreme peripheral node. At the end of the second BFS traversal, we would land on another extreme peripheral node. The distance that we traverse would be the diameter of the graph, according to the definition.
        * We can use BFS to find one extreme first, then call BFS again to find the distance from this extreme to the other.
    3. Another concept that is closely related to the concept of diameter is called centroid. Intuitively, the centroid nodes are the ones that are situated in the center of a graph. More precisely, the distance from the centroid to other nodes in the graph should be overall minimal, which is the opposite of the extreme peripheral nodes that we defined before.
        * Indeed, if we could identify the centroid of a graph, then the distance from this centroid to any of its extreme peripheral nodes would be half of the diameter of the graph.
        * There would be either one or two centroids in a tree-alike graph, and one can find a detailed proof in the solution of the [Minimum Height Tree].
        * If there is only one centroid, the diameter would be exactly twice of the distance between the centroid and any of extreme peripheral nodes.
        * If there are two centroids, the diameter would be one plus the distance between a pair of centroid and extreme peripheral node, by taking into account the distance between the two centroids.
        * In order to identify the centroids, we could apply the **topological sorting algorithm** (BFS) here. The main idea is that we start from the peripheral nodes (which are also known as leaf nodes), then we trim the nodes off layer by layer, as if we are peel an "onion" till we reach its core (i.e. centroids).
        * In certain sense, we could also consider the above topological sorting as a sort of BFS Traversal, where we traverse the graph from the outer most layer of the graph (i.e. leaf nodes), level by level, into its inner most layer (i.e. centroids).




[LeetCode Link]: https://leetcode.com/problems/making-a-large-island/
[LC505 The Maze II]: https://leetcode.com/problems/the-maze-ii/
[LC1197 Minimum Knight Moves]: https://leetcode.com/problems/minimum-knight-moves/
[LC310 Minimum Height Tree]: https://leetcode.com/problems/minimum-height-trees/solution/
[why-dfs-and-not-bfs-for-finding-cycle-in-graphs]: https://stackoverflow.com/questions/2869647/why-dfs-and-not-bfs-for-finding-cycle-in-graphs
[LC499 The Maze III]: https://leetcode.com/problems/the-maze-iii/
[LC1168 Optimize Water Distribution in a Village]: https://leetcode.com/problems/optimize-water-distribution-in-a-village/
[LC64. Minimum Path Sum]: https://leetcode.com/problems/minimum-path-sum/
[LC200. Number of Islands]: https://leetcode.com/problems/number-of-islands/