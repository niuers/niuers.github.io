---
title: "Graph"
date: 2021-04-30
categories:
  - blog
tags:
  - algorithm
  - graph
  - summary
---

1. Definitions and Implementations:
    1. A graph is connected if there is a path from every vertex to every other vertex in the graph.
    2. Acyclic graph: a graph with no cycles.
    3. A Tree: is an acyclic connected graph.
    4. Spanning Tree: A spanning tree of a graph is a connected subgraph with no cycles that include all vertices.
    5. For simple graph (at most one edge between any pair of vertices), the asympotic relationship between `V` and `E` is: `E=O(V^2)`, the upper bound is achieved in a complete graph.
    6. Adjacency-lists (linked list) data structure to represent the edges for vertex

2. Directed Graph (Digraph)
    1. Reverse a digraph
    2. Multiple-source reachability. Given a digraph and a set of source vertices, support queries of the form "Is there a directed path from any vertex in the set to a given target vertex v?" (Mark-and-sweep garbage collection)
        * DFS
    3. Scheduling problems: Precedence-constrained scheduling (Topological sort)
        * Given a digraph, put the vertices in order such that all its directed edges point from a vertex earlier in the order to a vertex later in the order (or report that doing so is not possible)
        * It is based on the idea that depth-first search visits each vertex exactly once. If we save the vertex given as argument to the recursive `dfs()` in a data structure, then iterate through that data structure, we see all the graph vertices, in order determined by the nature of the data structure and by whether we do the save before or after the recursive calls. Three vertex orderings are of interest in typical applications:
            * Preorder: Put the vertex on a queue before the recursive calls
            * Postorder : Put the vertex on a queue after the recursive calls
            * Reverse postorder (Topologic sort) : Put the vertex on a stack after the recursive calls (`V+E`)
    4. Cycles in digraphs (Directed cycle detection)
        * Does a given digraph have a directed cycle? If so, find the vertices on some such cycle, in order from some vertex back to itself. i.e. Is it a DAG? 
        * A directed acyclic graph (DAG) is a digraph with no directed cycles.
        * DFS: It uses a boolean array `onStack[]` to keep track of the vertices for which the recursive call has not completed. When it finds an edge `v->w` to a vertex `w` that is on the stack, it has discovered a directed cycle, which it can recover by following `edgeTo[]` links.
        * The node-coloring variant of the algorithm, The idea is to do DFS of a given graph and while doing traversal, assign one of the below three colors to every vertex.
            * WHITE ~ Vertex is not processed yet. Initially, all vertices are WHITE.
            * GRAY ~ Vertex is being processed (DFS for this vertex has started, but not finished which means that all descendants (in DFS tree) of this vertex are not processed yet (or this vertex is in the function call stack).
            * BLACK ~ Vertex and all its descendants are processed.
            * While doing DFS, if an edge is encountered from current vertex to a GRAY vertex, then this edge is a back edge and hence there is a cycle. A GRAY node represents a node whose processing is still ongoing. Thus, if a descendent eventually leads back to a node whose processing is ongoing, it ends up creating a cycle in the directed graph and we call the edge leading back to a GRAY node as a backward edge.
            * There are three possibilities for the color of this current node.
                * If it is BLACK, do nothing; we've already processed it.
                * If it is WHITE, then mark it as GRAY since we are starting the processing of the graph rooted at this node.
                * Otherwise, if it is GRAY, then return false as we have discovered a backward edge, and hence a cycle.
        * Why not Breadth-First Search?
            * From [this Stack Overflow answer]:
            * A BFS could be reasonable if the graph is undirected (be my guest at showing an efficient algorithm using BFS that would report the cycles in a directed graph!), where each cross edge defines a cycle (edge going from a node to an already visited node). If the cross edge is {v1, v2}, and the root (in the BFS tree) that contains those nodes is r, then the cycle is r ~ v1 - v2 ~ r (~ is a path, - a single edge), which can be reported almost as easily as in DFS.
            * The only reason to use a BFS would be if you know your (undirected) graph is going to have long paths and small path cover (in other words, deep and narrow). In that case, BFS would require proportionally less memory for its queue than DFS' stack (both still linear of course).
            * In all other cases, DFS is clearly the winner.


2. Depth-First Search (DFS)
    1. Recursive:
        * Complexity: DFS marks all the vertices connected to a given source in time proportional to the sume of their degrees (`2E`). 
            * To run DFS on all vertices of a graph (so we need visit each vertex once at least), the complexity is `V+E`.
        * We encounter each edge in the graph twice (once at each of its vertices)
        * Careful not to repeat vertices
        * Uses LIFO stack
    2. Single-source connectivity: Are two given vertices connected (path detection) ? and How many connected components does the graph have ?
        * DFS: `V+E`
        * Union-Find: however it can't find the path
            * It's an online algorithm: we can check whether two vertices are connected in near-constant time at any point, even while adding edges
    3. Single-source paths (Finding Paths). Given a graph and a source vertex s, support queries of the form "Is there a path from s to a given target vertex v? If so, find such a path".
        * DFS
            * It maintains a vertex-indexed array `edgeTo[]` or `parent[]` such that `edgeTo[w] = v` means that `v-w` was the edge used to access `w` for the first time. The `edgeTo[]` array is a parent-link representation of a tree rooted at `s` that contains all the vertices connected to `s`. Instead of just keeping track of the path from the current vertex back to the start
            * The paths discovered by depth-first search depend not just on the graph, but also on the representation and the nature of the recursion.    
    4. Two-colorability: Can the vertices of a given graph be assigned one of two colors in such a way that no edge connects vertices of the same color ? which is equivalent to this question: Is the graph bipartite ?
    5. Edge Classification
        * Tree edges: (parent pointer in DFS), the edges that are used in DFS to visit new vertex. They form a tree (and forest).
        * Forward edges
        * Backward edges: if a vertex is already on the stack, then current edge is back edge
        * Cross edges
        * In an undirected graph, only tree edges and backward edges can exist. There are no forward/cross edges.
    6. Cycle detection        
        * A graph `G` has a cycle if and only if the depth-first search of `G` has a back edge.
        * 
    7. Topological Sort
        * It usually applies on a DAG, so there's no back edge (no cycle). 
        * Run DFS output reverse of finishing times (the time after we finished processing all of its adjcent vertices) of vertices
        * BFS (Kahn's Algorithm): 
            * Start from nodes with in-degree of 0, and move to inside nodes, ech time need find the new nodes that with updated in-degree of 0.
            * `O(V+E)`

3. Breadth-First-Search (BFS)
    1. The goal is to visit all the nodes reachable from a given source node `s`
        1. It uses a FIFO queue
        2. BFS takes time proportional to `V+E` in the worst case. You only check each edge once (`|E|` in directed graph) or twice (`2|E|` in undirected graph), and a vertex enters the queue only once. 
        3. Need to be carful to avoid duplicates by using a 'visited' set
            * Be careful on where to add the vertex into the 'visited' set though. 
            * Shall we add a vertex into the 'visited' set when we pop it out of the queue?
            * Shall we add a neighbor of a vertex into the 'visited' set before appending it onto the queue while we loop through a vertex's neighbors? 
            * The latter is more efficient as we avoid adding many more vertices into the queue. If we use the former, the distance between each visited vertex can be much larger. See [Minimum Knight Moves][LC1197 Minimum Knight Moves].
        4. "Pruning" can be used to improve the efficency of BFS. e.g. We can use bidirectional BFS instead of unidirectional BFS. or We can remove unwanted vertices from the queue.
    2. **Single-source shortest paths**. Given a graph and a source vertex `s`, support queries of the form "Is there a path from s to a given target vertex v? If so, find a shortest such path (one with a minimal number of edges, not necessarily the only one)"
        * BFS: It also maintains `edgeTo[]` or `parent[]` to save the paths
    3. Dijkstraâ€™s algorithm solves the **single-source shortest-paths problem in edge-weighted digraphs with nonnegative weights**.
        * All-pairs shortest paths. Given an edge-weighted digraph, support queries of the form Given a source vertex s and a target vertex t, is there a path from s to t? If so, find a shortest such path (one whose total weight is minimal).

4. Minimum Spanning Tree (MST) of Edge-Weighted Undirected Graph
    0. A minimum spanning tree (MST ) of an edge-weighted graph is a spanning tree whose weight (the sum of the weights of its edges) is no larger than the weight of any other spanning tree.
    1. Adding an edge that connects two vertices in a tree creates a unique cycle
    2. Removing an edge from a tree breaks it into two separate subtrees
    3. Cut Property: A cut of a graph is a partition of its vertices into two nonempty disjoint sets. A crossing edge of a cut is an edge that connects a vertex in one set with a vertex in the other.
        * Given any cut in an edge-weighted graph, the crossing edge of minimum weight is in the MST of the graph.
        * Note that there is no requirement that the minimal edge be the only MST edge connect- ing the two sets; indeed, for typical cuts there are several MST edges that connect a vertex in one set with a vertex in the other
    4. The edge weights are all different. If edges can have equal weights, the minimum spanning tree may not be unique.
    5. Greedy Algorithm
        1. The following method colors black all edges in the the MST of any connected edge-weighted graph with V vertices: starting with all edges colored gray, find a cut with no black edges, color its minimum-weight edge black, and continue until V?1 edges have been colored black.
        2. Prim's Algorithm
            1. Use priority queue to hold crossing edges and find the cut with minimum weight
            2. We can use eager algorithm, we maintain on the priority queue just one edge for each non-tree vertex w: the shortest edge that connects it to the tree.
            2. Time complexity: `ElogE` (lazy version), `E log V` (eager version)
        3. Kruskal's Algorithm
            1. It processes the edges in order of their weight values (smallest to largest), taking for the MST (coloring black) each edge that does not form a cycle with edges previously added, stopping after adding V-1 edges have been taken.
            2. It use a priority queue to consider the edges in order by weight, a **union-find** data structure to identify those that cause cycles, and a queue to collect the MST edges.
            3. Space: `E`, Time: `ElogE`

6. The shortest path (i.e. path with the minimum total weights) from a source
    1. Example: [The Maze II ][LC505 The Maze II]
    2. Optimal Substructure
        * Subpaths of a shortest path are shortest paths
    3. Algorithms
        * Dijkstra Algorithm
        * Bellmen-Ford Algorithm
            * Works on edge with both positive and negative weights
            * `O(VE)`
            * Negative weight cycle: need detect and mark the related vertices with infinity cost
        * General Structure of the shortest path problem (no negative cycles)
            * Initialize: set source distance `d[s]=0`, and for each vertex `u`, set distance `d[u]`(The length of the current shortest path from source `s` to `u`) to `infinity`, and predecessor `p[u]` (The predecessor of `u` in the shortest path from `s` to `u`) as `None`
            * Repeat: select some edge `(u,v)` in some way (Algorithm specific)
            * **Relaxation** of `edge(u,v)` (from `u` to `v`): 
            ```
            if d[v] > d[u] + w(u,v):
                d[v] = d[u] + w(u,v)
                p[v] = u #predecessor
            ```
            * Until all edges have `d[v] <= d[u] + w(u,v)` (brutal force)
            * This algorithm depends on the ordering of vertices we choose which can be exponentially time complexity
    4. Dijkstra Algorithm (Greedy)
        * It assumes nonnegative weights  
        * It works with graph with cycles
        * The algorithm works without using 'visited' set to record the nodes that have been visited, because the nodes that we should skip already have the shortest distance, so they don't satisfy the condition to be pushed into the priority queue again. 
            * Note, if you use 'visited' set while looping through the neighbors, this is wrong. Because not all neighbors will have the shortest distance in this round, and they may need to be revisited to update their distances to the source.
        * Complexity: `O(VlogV +E)`, practically linear time, mostly dominated by `E`. 
            * Note, the complexity doesn't depend on the dynamic range of the weights at all.
            * BFS and DFS aren't directly applicable to the shortest path problem for graph with weights because of the dynamic weights.
            * Operations in Dijkstra
                * `O(V)`: insertion into priority queue (ADT)
                    * array: `O(V)`
                    * min-heap: `O(logV)`
                * `O(V)`: extract-min from priority queue 
                    * array: `O(V)`
                    * min-heap: `O(logV)` extract min
                * `O(E)`: relax (decrease key) operation on each vertex            
                    * array: `O(1)`
                    * min-heap: `O(logV)` for relaxation
                * Total complexity:
                    * Array: `O(V^2+E)=O(V^2)`
                    * Heap: `O(VlogV + ElogV)` (Fibonacci heap can remove the logV in second term)
        * Need keep the predecessors leading up to current vertex

    5. The Shortest Path Algorithm for DAG (dynamic programming)
        * **Whenenver you have DAG, try topological sort the DAG**, path from `u` to `v` implies that `u` is before `v` in ordering.
            * There's no cycles
            * You can start from any vertex
        * One pass (left to right) over the vertices in topologically sorted order,  relax each edge that leaves each vertex we are processing right now.
        * `O(V+E)`
        * This works with negative weights, but not with negative cycle


7. The Longest Path in the Graph (Diameter of Graph)
    1. We can loop through each node, and find the longest path from the node to any other node. It may take too long. It turns out that we don't have to run it for each node.
        * First of all, we will use the concepts of **leaf** nodes and **parent** nodes as in a Tree data structure. For a parent node, if we could obtain two longest distances (denoted as t1 and t2) starting from this parent node to any of its descendant leaf nodes, then the longest path that traverse this parent node would be `t1 + t2`.
        * Since any node in the graph has the potential to be part of the path that forms the diameter of the graph, we can iterate through each node to obtain all the longest paths as we defined shortly before. The diameter of the graph would be the maximum among all the longest paths that traverse each node.
        *

    2. According to the definition, the problem could be solved if we could identify the two nodes that have the longest distance among all. Let us refer to these two nodes as the **extreme peripheral** nodes.
        * First of all, we assert that starting from any node in the graph, if we run a BFS traversal, the last node that we visit would be one of the extreme nodes. An intuition that supports the above assertion is that as an extreme peripheral node, it should be the one that is far away from any of the other nodes in the graph. Given any node, the longest distance that starts from this node must end with one of the extreme peripheral nodes.
        * Once we identify one of the extreme peripheral nodes, we then could apply again the BFS traversal. But this time, we would start from the identified extreme peripheral node. At the end of the second BFS traversal, we would land on another extreme peripheral node. The distance that we traverse would be the diameter of the graph, according to the definition.
        * We can use BFS to find one extreme first, then call BFS again to find the distance from this extreme to the other.
    3. Another concept that is closely related to the concept of diameter is called centroid. Intuitively, the centroid nodes are the ones that are situated in the center of a graph. More precisely, the distance from the centroid to other nodes in the graph should be overall minimal, which is the opposite of the extreme peripheral nodes that we defined before.
        * Indeed, if we could identify the centroid of a graph, then the distance from this centroid to any of its extreme peripheral nodes would be half of the diameter of the graph.
        * There would be either one or two centroids in a tree-alike graph, and one can find a detailed proof in the solution of the [Minimum Height Tree].
        * If there is only one centroid, the diameter would be exactly twice of the distance between the centroid and any of extreme peripheral nodes.
        * If there are two centroids, the diameter would be one plus the distance between a pair of centroid and extreme peripheral node, by taking into account the distance between the two centroids.
        * In order to identify the centroids, we could apply the **topological sorting algorithm** (BFS) here. The main idea is that we start from the peripheral nodes (which are also known as leaf nodes), then we trim the nodes off layer by layer, as if we are peel an "onion" till we reach its core (i.e. centroids).
        * In certain sense, we could also consider the above topological sorting as a sort of BFS Traversal, where we traverse the graph from the outer most layer of the graph (i.e. leaf nodes), level by level, into its inner most layer (i.e. centroids).













 




[LeetCode Link]: https://leetcode.com/problems/making-a-large-island/
[LC505 The Maze II]: https://leetcode.com/problems/the-maze-ii/
[LC1197 Minimum Knight Moves]: https://leetcode.com/problems/minimum-knight-moves/
[LC310 Minimum Height Tree]: https://leetcode.com/problems/minimum-height-trees/solution/
[this Stack Overflow answer]: https://stackoverflow.com/questions/2869647/why-dfs-and-not-bfs-for-finding-cycle-in-graphs